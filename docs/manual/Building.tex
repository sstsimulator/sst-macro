%% !TEX root = manual.tex

\chapter{Building and Running SST/macro}
\label{chapter:building}

\section{Build and Installation of \sstmacro}
\label{sec:buildinstall}


\subsection{Downloading}
\label{subsec:build:downloading}

\sstmacro is available at \url{https://github.com/sstsimulator/sst-macro}

\begin{ShellCmd}
shell> git clone https://github.com/sstsimulator/sst-macro.git 
\end{ShellCmd}
or for ssh

\begin{ShellCmd}
shell> git clone ssh://git@github.com/sstsimulator/sst-macro.git 
\end{ShellCmd}


If you'd like to use ssh for convenience, make sure you have added a public key for your GitHub account.

\subsection{Dependencies}
\label{subsec:build:dependencies}

The following are dependencies for \sstmacro.

\begin{itemize}
\item (optional) Git is needed in order to clone the source code repository, but you can also download a tar (Section \ref{subsec:build:downloading}).
\item (optional, recommended) Autoconf and related tools are needed unless you are using an unmodified release or snapshot tar archive.
\item Autoconf: 2.68 or later 
\item Automake: 1.11.1 or later 
\item Libtool: 2.4 or later 
\item A C/C++ compiler is required with C++11 support.  gcc 4.8 and onward is known to work. 
\item (optional) Doxygen and Graphviz are needed to build the documentation.
\item (optional) Graphviz is needed to collect call graphs.
%\item (optional, recommended) Qt libraries and build system (qmake) are needed to build the GUI input configuration tool.
%Qt 5.0 and above are suggested, although 4.9 has been observed in the wild to work (Section \ref{sec:building:gui}).
%\item (optional) VTK is needed for advanced vis features.
\end{itemize}

\subsection{Configuration and Building}
\label{subsec:build:configure}

Once \sstmacro is extracted to a directory, we recommend the following as a baseline configuration, including building outside the source tree:

\begin{ShellCmd}
sst-macro> ./bootstrap.sh
sst-macro> mkdir build
sst-macro> cd build
sst-macro/build> ../configure --prefix=/path-to-install 
\end{ShellCmd}


A complete list of options can be seen by running \inlineshell{../configure --help}.   Some common options:

\begin{itemize}
\item --(dis|en)able-cpp11 : Controls whether features requiring C++11 language features are compiled. Feature is enabled by default (reverts to disabled if C++11 features are not available).
\item --with-boost=\textless path \textgreater: Use Boost.
The \textless path \textgreater argument is optional. If not given, default system paths like /usr/local will be searched for a Boost installation.
Alternatively, if \textless path \textgreater is omitted and the \$BOOST\_ROOT environment variable is defined, the Boost source tree will be directly used
instead of an installation directory.
\item --(dis|en)able-graphviz : Enables the collection of simulated call graphs, which can be viewed with graphviz.
%\item --with-qt=\$QMAKE: Direct \sstmacro to the qmake executable.  
%If no value is specified, it just assumes qmake is in your \$PATH  (see Section \ref{sec:building:gui}).
\item --(dis|en)able-unordered-containers : Unordered containers can provide better performance for large maps and sets.
Enabled by default. Disable if not using Boost or C++11. Ordered maps can be used as a replacement, but with lower performance.
\item --(dis|en)able-regex : Regular expressions can be used to proofread input files, but this requires either Boost or C++11.
Enabled by default. Disable if not using Boost or C++11.
\item --(dis|en)able-custom-new : Memory is allocated in larger chunks in the simulator, which can speed up large simulations.
\item --(dis|en)able-fortran : Enable support for running fortran skeletons.
\item --(dis|en)able-mpiparallel: Enable parallel discrete event simulation in distributed memory over MPI.  See Section \ref{sec:PDES}.
\item --(dis|en)able-replacement-headers : This configures a compiler wrapper that will automagically compile existing MPI/pThread code for simulation, rather than actual execution. This is off by default, but actually recommended.
\item --(dis|en)able-multithread : This configures for thread-level parallelism for (hopefully) faster simulation
\end{itemize}

Once configuration has completed, printing a summary of the things it found, simply type \inlineshell{make}.  
It is recommended to use the -j option for a parallel build with as many cores as you have (otherwise it will take quite a while).  

\subsubsection{Boost vs. C++11}
\sstmacro now requires either C++11 support or the use of boost for the best performance and full error checking.
On some compilers (particularly GCC 4.8), C++11 regular expressions may not work.  In this case, regular expression use can be disabled using the \inlineshell{--disable-regex} configure flag. Input files will not be proofread for errors, which an lead to unexplained behavior from input file typos.  There are many parameters with default values meaning that a type-o will not cause an error - instead it will cause the default value to be used. 

GCC compilers with C++11 support are known to work with the following known exceptions:

\begin{itemize}
\item GCC 4.9 with C++11 can produce stack alignment errors for certain statistic tests.
This error only occurs on the Mac OS X version. The vast majority of tests still pass.
\item GCC 4.8 with C++11 has bugs in regular expression parsing on RHEL 6.
\end{itemize}

It is possible to use neither Boost nor C++11.
As before, the \inlineshell{--disable-regex} flag must be used since there is no regular expression support built into C++98/03.
In addition, \sstmacro leverages unordered map/set containers heavily within the code, which are only available in Boost and C++11.
The requirement for unordered containers can be avoided.
Use the \inlineshell{--disable-unordered-containers} configure flag.
While the code will not be as efficient as a Boost or C++11 build,
it will still run correctly with decent performance.

%TODO building with SST/micro backend

\subsection{Post-Build}
\label{subsec:postbuild}

If the build did not succeed, check \ref{subsec:build:issues} for known issues, or contact \sstmacro support for help (sstmacro-support@googlegroups.com).

If the build was successful, it is recommended to run the range of tests to make sure nothing went wrong.  
To do this, and also install \sstmacro  to the install path specified during installation, run the following commands:

\begin{ShellCmd}
sst-macro/build> make -j8 check
sst-macro/build> sudo make install
sst-macro/build> export PATH=$PATH:/path-to-install
sst-macro/build> make -j8 installcheck
\end{ShellCmd}
Make check runs all the tests we use for development, which checks all functionality of the simulator.  
Make installcheck compiles some of the skeletons that come with \sstmacro, linking against the installation.  

\aside{
Important:  Applications and other code linking to \sstmacro use Makefiles that use the sst++/sstcc compiler wrappers
that are installed there for convenience to figure out where headers and libraries are.  Make sure your path is properly configured.
}

\subsection{Known Issues}
\label{subsec:build:issues}


\begin{itemize}
\item Any build or runtime problems should be reported to sstmacro-devel@googlegroups.com.  We try to respond as quickly as possible.
\item make -j: When doing a parallel compile dependency problems can occur.  
There are a lot of inter-related libraries and files.  
Sometimes the Makefile dependency tracker gets ahead of itself and you will get errors about missing libraries and header files.
If this occurs, restart the compilation.  If the error vanishes, it was a parallel dependency problem.
If the error persists, then it's a real bug.

\item Ubuntu: The Ubuntu linker performs too much optimization on dynamically linked executables.
Some call it a feature.  I call it a bug.
In the process it throws away symbols it actually needs later.
When compiling with Ubuntu, make sure that '-Wl,--no-as-needed' is always given in LDFLAGS.

The problem occurs when the executable depends on libA which depends on libB.
The executable has no direct dependence on any symbols in libB.
Even if you add \inlineshell{-lB} to the \inlineshell{LDFLAGS} or \inlineshell{LDADD} variables,
the linker ignores them and throws the library out.
It takes a dirty hack to force the linkage.
If there are issues, contact the developers at sstmacro-devel@googlegroups.com and report the problem. 
It can be fixed easily enough.

\item Compilation with clang should work, although the compiler is very sensitive.  
In particular, template code which is correct and compiles on several other platforms can mysteriously fail.  Tread with caution.
\end{itemize}

\section{Building DUMPI}
\label{sec:building:dumpi}

By default, DUMPI is configured and built along with SST/macro with support for reading and parsing DUMPI traces, known as libundumpi.  
DUMPI binaries and libraries are also installed along with everything for \sstmacro during make install.   
DUMPI can be used as it's own library within the \sstmacro source tree by changing to \inlineshell{sst-macro/sst-dumpi}, 
where you can change its configuration options.  
It is not recommended to disable libundumpi support, which wouldn't make much sense anyway. 

DUMPI can also be used as stand-alone tool/library if you wish (\eg~for simplicity if you're only tracing). 
To get DUMPI by itself, either copy the \inlineshell{sstmacro/dumpi} directory somewhere else or visit \url{https://github.com/sstsimulator/sst-dumpi} and follow similar instructions for obtaining \sstmacro.

To see a list of configuration options for DUMPI, run \inlineshell{./configure --help}.  
If you're trying to configure DUMPI for trace collection, use \inlineshell{--enable-libdumpi}.
Your build process might look like this (if you're building in a separate directory from the dumpi source tree) :

\begin{ShellCmd}
sst-dumpi/build> ../configure --prefix=/path-to-install --enable-libdumpi
sst-dumpi/build> make -j8
sst-dumpi/build> sudo make install
\end{ShellCmd}

\subsection{Known Issues}
\label{subsubsec:building:dumpi:issues}

\begin{itemize}
\item When compiling on platforms with compiler/linker wrappers, e.g. ftn (Fortran) and CC (C++) compilers 
at NERSC, the libtool configuration can get corrupted.  The linker flags automatically added by the 
wrapper produce bad values for the predeps/postdeps variable in the libtool script in the top 
level source folder.  When this occurs, the (unfortunately) easiest way to fix this is to manually modify
the libtool script.  Search for predeps/postdeps and set the values to empty.
This will clear all the erroneous linker flags.  The compilation/linkage should still work since 
all necessary flags are set by the wrappers. 
\end{itemize}

\section{Running an Application}
\label{sec:tutorial:runapp}

To demonstrate how an application is run in \sstmacro, we'll use a very simple send-recv program located in \inlineshell{skeletons/sendrecv}.
We will take a closer look at the actual code in Section \ref{sec:tutorial:basicmpi}.
After \sstmacro has been installed and your PATH variable set correctly, run: 

\begin{ShellCmd}
sst-macro> cd skeletons/sendrecv
sst-macro/skeleton/sendrecv> make
sst-macro/skeleton/sendrecv> ./runsstmac -f parameters.ini
\end{ShellCmd}

You should see some output that tells you 1) the estimated total (simulated) runtime of the simulation, and 
2) the wall-time that it took for the simulation to run.  
Both of these numbers should be small since it's a trivial program. 

This is how simulations generally work in \sstmacro: you build skeleton code and link it with the simulator to produce a binary.  
Then you run that binary and pass it a parameter file which describes the machine model to use.

\subsection{Makefiles}
\label{subsec:tutorial:makefiles}

We recommend structuring the Makefile for your project like the one seen in \inlineshell{skeletons/sendrecv/Makefile} :

\begin{ViFile}
TARGET := runsstmac
SRC := $(shell ls *.c) 

CXX :=      $(PATH_TO_SST)/bin/sst++
CC :=        $(PATH_TO_SST)/bin/sstcc
CXXFLAGS := ...
CPPFLAGS := ...
LIBDIR :=  ...
PREFIX :=   ...
LDFLAGS :=  -Wl,-rpath,$(PREFIX)/lib
...
\end{ViFile}
The SST compiler wrappers \inlineshell{sst++} and \inlineshell{sstcc} automagically configure and map the code for simulation.  More details are given in Section \ref{sec:skel:linkage}.

\subsection{Command-line arguments}
\label{subsec:tutorial:cmdline}

There are only a few basic command-line arguments you'll ever need to use with \sstmacro, listed below

\begin{itemize}
\item -h/--help: Print some typical help info
\item -f [parameter file]: The parameter file to use for the simulation.  
This can be relative to the current directory, an absolute path, or the name of a pre-set file that is in sstmacro/configurations 
(which installs to /path-to-install/include/configurations, and gets searched along with current directory). 
\item --dumpi: If you are in a folder with all the DUMPI traces, you can invoke the main \inlinecode{sstmac} executable with this option.  This replays the trace in a special debug mode for quickly validating the correctness of a trace.
\item -d [debug flags]: A list of debug flags to activate as a comma-separated list (no spaces) - see Section \ref{sec:dbgoutput}
\item -p [parameter]=[value]: Setting a parameter value (overrides what is in the parameter file)
\item -t [value]: Stop the simulation at simulated time [value]
\item -c: If multithreaded, give a comma-separated list (no spaces) of the core affinities to use - see Section \ref{subsec:parallelopt}
\end{itemize}

\section{Parallel Simulations}
\label{sec:PDES}

\sstmacro supports running parallel discrete event simulation (PDES) in distributed memory (MPI), threaded shared memory (pthreads) and hybrid (MPI+pthreads) modes.  

\subsection{Distributed Memory Parallel}
\label{subsec:mpiparallel}
In order to run distributed memory parallel, you must configure the simulator with the \inlineshell{--enable-mpiparallel} flag.
Configure will check for MPI and ensure that you're using
the standard MPI compilers.  Your configure should look something like:

\begin{ShellCmd}
sst-macro/build> ../configure --enable-mpiparallel CXX=mpicxx CC=mpicc ...
\end{ShellCmd}

\sstmacro can now run parallel jobs without any graph partitioning packages.
In previous versions, \sstmacro required METIS for partitioning the workload amongst parallel processes.
With the above options, you can just compile and go.
%For some cases, you may wish to use METIS for a more intelligent partitioning of the nodes to improve parallel performance.
%Make sure that you have installed \inlineshell{gpmetis} somewhere and the binary is in your PATH. 
%METIS can be found at \url{http://glaros.dtc.umn.edu/gkhome/metis/metis/download}.  
\sstmacro is run exactly like the serial version, but is spawned like any other MPI parallel program.
Use your favorite MPI launcher to run, e.g. for OpenMPI

\begin{ShellCmd}
mysim> mpirun -n 4 sstmac -f parameters.ini
\end{ShellCmd}

or for MPICH

\begin{ShellCmd}
mysim$ mpiexec -n 4 sstmac -f parameters.ini
\end{ShellCmd}

If you set the --enable-mpiparallel flag for configure and use an MPI compiler,
\sstmacro should by default correctly configure everything to run in parallel.
If you are having issues, then check the following.
You might have to set the environment variable \inlineshell{SSTMAC_PARALLEL=mpi}.
If another value like \inlineshell{SSTMAC_PARALLEL=serial} is set, the run will fail.
No changes to the input file should be necessary. 
\begin{itemize}
\item \inlinefile{transport = mpi}: for non-MPI compilation \sstmacro assumes \inlinefile{transport = serial}.
\item \inlinefile{event_manager = clock_cycle_parallel}:  for non-MPI compilation \sstmacro uses a serial event map.  
Currently, only one form of parallelism is supported - conservative, clock-cycle parallelism.  
More types of parallel event managers may be available in future versions.
If running on a single processor, \inlinefile{clock_cycle_parallel} is functionally equivalent to a serial event map.
\item \inlinefile{partition = block}: this uses a simple block partitioning strategy.  
\end{itemize}

Even if you compile for MPI parallelism, the code can still be run in serial with the same configuration options.
\sstmacro will notice the total number of ranks is 1 and ignore any parallel options.
When launched with multiple MPI ranks, \sstmacro will automatically figure out how many partitions (MPI processes) 
you are using, partition the network topology into contiguous blocks, and start running in parallel.   

Naive block partitioning will create significant load imbalance when the number of unoccupied switches is large.
Adding \inlinefile{partition = occupied_block} to the input will force occupied and unoccupied switches to be partitioned separately, improving load balance between partitions.
However, at the partitioning stage of the simulation the simulator is actually unaware of the number of application tasks that will be created, so the \inlinefile{occupied_switches} parameter must be set to inform the partitioner how many switches will be occupied in the simulated network.

%If you installed it and want to try METIS, set 
%\inlinefile{partition = metis}.


\subsection{Shared Memory Parallel}
\label{subsec:parallelopt}
In order to run shared memory parallel, you must configure the simulator with the \inlineshell{--enable-multithread} flag.
Partitioning for threads is currently always done using block partitioning and there is no need to set an input parameter.
Including the integer parameter \inlineshell{sst_nthread} specifies the number of threads to be used (per rank in MPI+pthreads mode) in the simulation.
The following configuration options may provide better threaded performance.
\begin{itemize}
\item\inlineshell{--enable-spinlock} replaces pthread mutexes with spinlocks.  Higher performance and recommended when supported.
\item\inlineshell{--enable-cpu-affinity} causes \sstmacro to pin threads to specific cpu cores.  When enabled, \sstmacro will require the
\inlineshell{cpu_affinity} parameter, which is a comma separated list of cpu affinities for each MPI task on a node.  \sstmacro will sequentially
pin each thread spawned by a task to the next next higher core number.  For example, with two MPI tasks per node and four threads per MPI task,
\inlineshell{cpu_affinity = 0,4} will result in MPI tasks pinned to cores 0 and 4, with pthreads pinned to cores 1-3 and 5-7.
For a threaded only simulation \inlineshell{cpu_affinity = 4} would pin the main process to core 4 and any threads to cores 5 and up.
The affinities can also be specified on the command line using the \inlineshell{-c} option.
Job launchers may in some cases provide duplicate functionality and either method can be used.
\end{itemize}

\subsection{Warnings for Parallel Simulation}
\label{subsec:parallelwarn}
\begin{itemize}
\item Watch your \inlineshell{LD_LIBRARY_PATH} if you have multiple different builds. If your paths get scrambled and the wrong libraries are being read, you will get bizarre, inscrutable errors.
\item If the number of simulated processes specified by e.g. \inlinefile{aprun -n 100} does not match the number of nodes in the topology (i.e. you are not space-filling the whole simulated machine),
parallel performance will suffer. \sstmacro partitions nodes, not processes.
\item Furthermore, if you don't space-fill the simulated machine, you might even get weird errors. Some MPI ranks might have zero virtual processes, which leads to undefined behavior.
\item If running an MPI program,  you should probably be safe and use the mpicheck debug flag (see below) to ensure the simulation runs to completion.
The flag ensures \inlinecode{MPI_Finalize} is called and the simulation did not ``deadlock.''
While the PDES implementation should be stable, it's best to treat it as Beta++ to ensure program correctness.
\end{itemize}

\aside{
Parallel simulation speedups are likely to be modest for small runs.
Speeds are best with serious congestion or heavy interconnect traffic.
Weak scaling is usually achievable with 100-500 simulated MPI ranks per logical process.
Even without speedup, parallel simulation can certainly be useful in overcoming memory constraints, expanding the maximum memory footprint. 
}

\section{Debug Output}
\label{sec:dbgoutput}
\sstmacro defines a set of debug flags that can be specified in the parameter file to control debug output printed by the simulator.
To list the set of all valid flags with documentation, the user can run

\begin{ShellCmd}
bin> ./sstmac --debug-flags
\end{ShellCmd}

which will output something like

\begin{ViFile}
    mpi
        print all the basic operations that occur on each rank - only API calls are
        logged, not any implementation details
    mpi_check
        validation flag that performs various sanity checks to ensure MPI application
        runs and terminates cleanly
    mpi_collective
        print information about MPI collective calls as well as implementation details
    mpi_pt2pt
        print information about MPI point-to-point calls as well as implementation
        details
     ....
\end{ViFile}
The most important flag for validating simulations is the \inlineshell{mpi_check} flag,
which causes special sanity checks and a final validation check to ensure the simulation has finished cleanly.
Some of the debug flags can generate information overload and will only be useful to a serious developer, rather than a user.

To turn on debug output, add the following to the input file

\begin{ViFile}
debug = mpi  mpi_check
\end{ViFile}
listing all flags you want separated by spaces.
Note: this is a major shift from the previous (and really tedious, unfriendly) debug system of past versions.
The new system allows much finer-grained, simpler printing of debug output.
Additionally, it allows new debug flags to very easily defined.
More info on declaring new debug flags in your own code can be found in the developer's reference.
